# -*- coding: utf-8 -*-
import os
import re
import io
import time
from datetime import datetime, timezone, timedelta

import pandas as pd
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# ===== è¨­å®š =====
RELEASE_TAG = "news-latest"
ASSET_NAME = "yahoo_news.xlsx"
SHEET_NAMES = [
    "ãƒ›ãƒ³ãƒ€",
    "ãƒˆãƒ¨ã‚¿",
    "ãƒãƒ„ãƒ€",
    "ã‚¹ãƒãƒ«",
    "ãƒ€ã‚¤ãƒãƒ„",
    "ã‚¹ã‚ºã‚­",
    "ä¸‰è±è‡ªå‹•è»Š",
    "æ—¥ç”£",  # ä»»æ„ã§è¿½åŠ 
]

# æ—¢å®šã¯ä¸Šã®ãƒªã‚¹ãƒˆã€‚ç’°å¢ƒå¤‰æ•° NEWS_KEYWORDS ã«ã€Œãƒ›ãƒ³ãƒ€,ãƒˆãƒ¨ã‚¿,â€¦ã€ã¨å…¥ã‚ŒãŸã‚‰ä¸Šæ›¸ãå¯èƒ½
def get_keywords() -> list[str]:
    env = os.getenv("NEWS_KEYWORDS")
    if env:
        # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š or æ”¹è¡Œã§åˆ†å‰²
        parts = [p.strip() for p in re.split(r"[,\n]", env) if p.strip()]
        return parts or SHEET_NAMES
    return SHEET_NAMES


# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def jst_now():
    return datetime.now(timezone(timedelta(hours=9)))


def jst_str(fmt="%Y/%m/%d %H:%M"):
    return jst_now().strftime(fmt)


# ===== Chromeï¼ˆheadlessï¼‰ =====
def make_driver() -> webdriver.Chrome:
    opts = Options()
    chrome_path = os.getenv("CHROME_PATH")  # Actionsã§æ³¨å…¥
    if chrome_path:
        opts.binary_location = chrome_path
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1280,2000")
    # é•·æœŸé‹ç”¨æ™‚ã®å‡ºã—åˆ†ã‘å¯¾ç­–ï¼šUAå›ºå®šã§ã‚‚è‰¯ã„ãŒã€å¤ã™ãã‚‹ã¨è¦ç´ å‡ºã—åˆ†ã‘ãŒèµ·ãã‚‹å ´åˆã‚ã‚Š
    opts.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)


# ===== å¼•ç”¨å…ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— =====
DATE_RE = re.compile(r"(?:\d{4}/\d{1,2}/\d{1,2}|\d{1,2}/\d{1,2})\s*\d{1,2}[:ï¼š]\d{2}")


def clean_source_text(text: str) -> str:
    if not text:
        return ""
    t = text
    t = re.sub(r"[ï¼ˆ(][^ï¼‰)]+[ï¼‰)]", "", t)      # ï¼ˆï¼‰å†…ã‚’å‰Šé™¤
    t = DATE_RE.sub("", t)                       # æ—¥ä»˜+æ™‚åˆ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤
    t = re.sub(r"^\d+\s*", "", t)                # å…ˆé ­ã®é€šã—ç•ªå·ï¼ˆä¾‹: "2 Merkmal"ï¼‰
    t = re.sub(r"\s{2,}", " ", t).strip()        # ä½™åˆ†ãªç©ºç™½æ•´ç†
    return t


# ===== Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ï¼ˆ1ãƒšãƒ¼ã‚¸åˆ†ï¼‰ =====
def scrape_yahoo(keyword: str) -> pd.DataFrame:
    """
    æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ¤œç´¢ï¼‰ã‹ã‚‰ ã‚¿ã‚¤ãƒˆãƒ«/URL/æŠ•ç¨¿æ—¥/å¼•ç”¨å…ƒ ã‚’å–å¾—ï¼ˆ1ãƒšãƒ¼ã‚¸ï¼‰
    """
    driver = make_driver()
    url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(url)
    time.sleep(5)  # åˆæœŸæç”»å¾…ã¡

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    # li ã®ã‚¯ãƒ©ã‚¹ã¯å¤‰å‹•ã—ã‚„ã™ã„ã®ã§æ­£è¦è¡¨ç¾ã§æ‹¾ã†
    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    rows = []
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            # æŠ•ç¨¿æ—¥ï¼šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãã‚Œã° "9/26 16:19" ã«æ­£è¦åŒ–
            pub_date = "å–å¾—ä¸å¯"
            if date_str:
                ds = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã€Œ2024/09/26 16:19ã€ã®å ´åˆ
                    dt = datetime.strptime(ds, "%Y/%m/%d %H:%M")
                    pub_date = dt.strftime("%-m/%-d %H:%M")
                except ValueError:
                    try:
                        # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã€Œ9/26 16:19ã€ã®å ´åˆ
                        dt = datetime.strptime(ds, "%m/%d %H:%M")
                        pub_date = dt.strftime("%-m/%-d %H:%M")
                    except Exception:
                        # ã©ã¡ã‚‰ã«ã‚‚å½“ã¦ã¯ã¾ã‚‰ãªã„å ´åˆã€å…ƒã®æ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨
                        pub_date = ds

            # å¼•ç”¨å…ƒï¼ˆåª’ä½“ï¼‹ã‚«ãƒ†ã‚´ãƒªãªã©ï¼‰ã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒªãƒ¼ãƒ³
            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div",
            ]:
                el = li.select_one(sel)
                if not el:
                    continue
                raw = el.get_text(" ", strip=True)
                txt = clean_source_text(raw)
                if txt and not txt.isdigit():
                    source = txt
                    break

            if title and url:
                rows.append(
                    {
                        "ã‚¿ã‚¤ãƒˆãƒ«": title,
                        "URL": url,
                        "æŠ•ç¨¿æ—¥": pub_date,
                        "å¼•ç”¨å…ƒ": source or "Yahoo",
                        "å–å¾—æ—¥æ™‚": jst_str(),       # è¿½è¨˜é‹ç”¨ã®ãŸã‚å–å¾—æ™‚åˆ»ã‚‚ä¿æŒ
                        "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,   # å¿µã®ãŸã‚åˆ—ã¨ã—ã¦ã‚‚æŒã£ã¦ãŠã
                    }
                )
        except Exception:
            continue

    return pd.DataFrame(rows, columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])


# ===== Releaseã‹ã‚‰æ—¢å­˜Excelã‚’å–å¾—ï¼ˆå…¨ã‚·ãƒ¼ãƒˆï¼‰ =====
def download_existing_book(repo: str, tag: str, asset_name: str, token: str) -> dict[str, pd.DataFrame]:
    """
    Release(tag)ã®æ—¢å­˜Excelå…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿å‡ºã—ã¦ {sheet_name: df} ã§è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€æŒ‡å®šã‚·ãƒ¼ãƒˆåãã‚Œãã‚Œç©ºDFã§è¿”ã™ã€‚
    """
    # åˆæœŸå€¤ï¼ˆæŒ‡å®šã®å…¨ã‚·ãƒ¼ãƒˆåˆ†ã€ç©ºDFï¼‰
    empty_cols = ["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
    dfs: dict[str, pd.DataFrame] = {sn: pd.DataFrame(columns=empty_cols) for sn in SHEET_NAMES}

    if not (repo and tag):
        print("âš ï¸ download_existing_book: repo/tag ãŒæœªè¨­å®šã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        return dfs

    base = "https://api.github.com"
    headers = {"Accept": "application/vnd.github+json"}
    # token ã¯ browser_download_url ã§ã¯ä¸è¦ã ãŒã€/releases èª­ã¿å‡ºã—ã«ã¯ã‚ã£ã¦ã‚‚OK
    if token:
        headers["Authorization"] = f"Bearer {token}"

    # 1) Release æƒ…å ±å–å¾—
    url_rel = f"{base}/repos/{repo}/releases/tags/{tag}"
    r = requests.get(url_rel, headers=headers)
    print(f"ğŸ” GET {url_rel} -> {r.status_code}")
    if r.status_code != 200:
        print("âš ï¸ ReleaseãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å–å¾—ã«å¤±æ•—ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs
    rel = r.json()

    # 2) å¯¾è±¡ã‚¢ã‚»ãƒƒãƒˆæ¢ç´¢
    asset = next((a for a in rel.get("assets", []) if a.get("name") == asset_name), None)
    if not asset:
        print(f"âš ï¸ Releaseã« {asset_name} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs

    # 3) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ browser_download_url ã‚’ä½¿ç”¨ï¼ˆèªè¨¼ä¸è¦ã§å®‰å®šï¼‰
    dl_url = asset.get("browser_download_url")
    if not dl_url:
        print("âš ï¸ browser_download_url ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs

    dr = requests.get(dl_url)
    print(f"â¬‡ï¸  Download {dl_url} -> {dr.status_code}, {len(dr.content)} bytes")
    if dr.status_code != 200:
        print("âš ï¸ æ—¢å­˜Excelã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€‚æ—¢å­˜ã¯ç©ºã¨ã—ã¦ç¶šè¡Œã—ã¾ã™ã€‚")
        return dfs

    # 4) Excel èª­ã¿è¾¼ã¿
    with io.BytesIO(dr.content) as bio:
        try:
            book = pd.read_excel(bio, sheet_name=None)
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜Excelã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return dfs

    # 5) ã‚·ãƒ¼ãƒˆã”ã¨ã«æ•´å½¢ã—ã¦è¿”ã™
    for sn in SHEET_NAMES:
        if sn in book:
            df = book[sn]
            # æ¬ ã‘ã¦ã„ã‚‹åˆ—ãŒã‚ã‚Œã°è£œå®Œï¼ˆå°†æ¥ã®åˆ—è¿½åŠ ã«ã‚‚è€æ€§ï¼‰
            for col in empty_cols:
                if col not in df.columns:
                    df[col] = ""
            dfs[sn] = df[empty_cols].copy()

    return dfs


# ===== Excelä¿å­˜ï¼ˆä½“è£èª¿æ•´ã¤ãï¼‰ =====
def save_book_with_format(dfs: dict[str, pd.DataFrame], path: str):
    from openpyxl import Workbook
    from openpyxl.utils import get_column_letter
    from openpyxl.styles import Font, Alignment

    wb = Workbook()
    # æ—¢å®šã§ä½œã‚‰ã‚Œã‚‹æœ€åˆã®ã‚·ãƒ¼ãƒˆã‚’å‰Šé™¤
    default_ws = wb.active
    wb.remove(default_ws)

    for sheet_name, df in dfs.items():
        ws = wb.create_sheet(title=sheet_name)
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = ["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
        ws.append(headers)
        # ãƒ‡ãƒ¼ã‚¿
        if not df.empty:
            for row in df[headers].itertuples(index=False, name=None):
                ws.append(list(row))

        # ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        max_col = ws.max_column
        max_row = ws.max_row
        ws.auto_filter.ref = f"A1:{get_column_letter(max_col)}{max_row}"

        # ãƒ˜ãƒƒãƒ€ãƒ¼å¤ªå­— & ç¸¦ä¸­å¤®
        header_font = Font(bold=True)
        for cell in ws[1]:
            cell.font = header_font
            cell.alignment = Alignment(vertical="center")

        # åˆ—å¹…ï¼ˆè»½èª¿æ•´ï¼‰
        widths = {
            "A": 50,  # ã‚¿ã‚¤ãƒˆãƒ«
            "B": 60,  # URL
            "C": 16,  # æŠ•ç¨¿æ—¥
            "D": 24,  # å¼•ç”¨å…ƒ
            "E": 16,  # å–å¾—æ—¥æ™‚
            "F": 16,  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        }
        for col, wdt in widths.items():
            if ws.max_column >= ord(col) - 64:
                ws.column_dimensions[col].width = wdt

        # 1è¡Œç›®å›ºå®š
        ws.freeze_panes = "A2"

    # å‡ºåŠ›
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    wb.save(path)


# ===== ãƒ¡ã‚¤ãƒ³ =====
def main():
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ç’°å¢ƒå¤‰æ•°NEWS_KEYWORDSã§ä¸Šæ›¸ãå¯èƒ½ï¼ˆä¾‹: "ãƒ›ãƒ³ãƒ€,ãƒˆãƒ¨ã‚¿,..."ï¼‰
    keywords = get_keywords()
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§: {', '.join(keywords)}")

    # 1) æ—¢å­˜ãƒ–ãƒƒã‚¯å–å¾—ï¼ˆå›ºå®šReleaseã‹ã‚‰ï¼‰
    token = os.getenv("GITHUB_TOKEN", "")
    repo = os.getenv("GITHUB_REPOSITORY", "")
    dfs_old = download_existing_book(repo, RELEASE_TAG, ASSET_NAME, token)

    # 2) æ–°è¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ— â†’ ã‚·ãƒ¼ãƒˆã”ã¨ã«ãƒãƒ¼ã‚¸ï¼ˆURLã§é‡è¤‡æ’é™¤ã€æ—¢å­˜å„ªå…ˆï¼æ–°è¦ã¯æœ«å°¾ï¼‰
    dfs_merged: dict[str, pd.DataFrame] = {}
    for kw in keywords:
        df_old = dfs_old.get(kw, pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ", "å–å¾—æ—¥æ™‚", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]))
        df_new = scrape_yahoo(kw)

        df_all = pd.concat([df_old, df_new], ignore_index=True)
        if not df_all.empty:
            df_all = df_all.dropna(subset=["URL"]).drop_duplicates(subset=["URL"], keep="first")
            # ä¸¦ã¹æ›¿ãˆã¯ã—ãªã„ï¼šæ—¢å­˜ã®ä¸¦ã³ã‚’ç¶­æŒã—ã€æ–°è¦ã¯æœ«å°¾ã«ä»˜ã
        dfs_merged[kw] = df_all

        print(f"  - {kw}: æ—¢å­˜ {len(df_old)} ä»¶ + æ–°è¦ {len(df_new)} ä»¶ â†’ åˆè¨ˆ {len(df_all)} ä»¶")

    # 3) ä¿å­˜ï¼ˆå„ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ã€ãƒ˜ãƒƒãƒ€ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ãƒ•ãƒªãƒ¼ã‚ºç­‰ï¼‰
    os.makedirs("output", exist_ok=True)
    out_path = os.path.join("output", ASSET_NAME)
    save_book_with_format(dfs_merged, out_path)

    print(f"âœ… Excelå‡ºåŠ›: {out_path}")
    # å›ºå®šDLãƒªãƒ³ã‚¯ï¼ˆå®Ÿãƒªãƒã‚¸ãƒˆãƒªåãŒåˆ†ã‹ã‚Œã°æ•´å½¢ï¼‰
    if repo:
        owner_repo = repo
    else:
        owner_repo = "<OWNER>/<REPO>"
    print(f"ğŸ”— å›ºå®šDL: https://github.com/{owner_repo}/releases/download/{RELEASE_TAG}/{ASSET_NAME}")


if __name__ == "__main__":
    main()
